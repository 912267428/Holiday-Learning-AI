## 集成学习：

​	<u>指通过构建多个弱学习器然后结合为一个强学习器来完成分类任务。</u>可以比单一的弱分类器的准确率更高。这些弱学习器可以是相同的算法（eg.决策树）也可以是使用不同算法的。

### 	集成学习的前提条件：

​		1.各分类器之间应该具有差异性。
​		2.每个分类器的精度应该大于0.5，否则将会负优化。

### 集成学习的分类：

​	1.Boosting（个体学习器之间存在强依赖关系）。

​	2.Bagging（个体学习器间不存在强依赖关系）
​	二者的取样方法不同。Bagging采用均匀取样，Boosting根据错误率来取样。
​	Boosting是迭代算法，每一次迭代都根据上一次的结果对样本进行加权，误差越来越小。无法并行运行。

## AdaBoost算法：

​	分类问题给定一个训练样本集，粗糙的分类规则（弱分类器）比精确的分类规则（强分类器）容易得到。AdaBoost算法就是从弱学习算法出发，反复学习，得到一系列的弱分类器，然后组合成一个强分类器。

### 	两个问题：

​		1.如何改变训练数据的权值或概论分布：提高前一轮弱分类器错误分类样本的权值，降低正确分类的权值
​		2.如何将弱分类器组合成强分类器：加权多数表决的方法。加大分类误差率小的弱分类器的权值，减小误差率较大的弱分类器的权值

### 	AdaBoost算法步骤：

​		1）初始化训练样本的权值分布。如果有N个岩本，最开始赋予相同的权值：1/N。
​		2）训练弱分类器。训练过程中如果莫格样本已经被正确地分类那么在构造得下一个训练集中他的权值会降低。相反，它的权值会提高。整个训练过程如果迭代地进行下去就会使得分类器逐步改进
​		3）将各个弱分类器组合成强分类器。误差率低的弱分类器在最终分类器中权重较大，否则较小。得到最终分类器。