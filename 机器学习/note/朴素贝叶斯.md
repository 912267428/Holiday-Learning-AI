## 朴素贝叶斯 (Naive Bayes) 

*基于贝叶斯定理与特征条件假设的分类方法。*
	对于给定的训练数据集，先基于特征条件独立假设学习输入输出的联合分布，然后基于此模型对给定的输入x利用贝叶斯定理求出概率最大的输出y

### 贝叶斯定理：

​	1.条件概率：p（A|B）=P(AB) / P(B)
​	2.贝叶斯定理就是基于条件概率，通过P(A|B)来求P(B|A):
​		p(B|A)=P(A|B)*P(B) / P(A)
​				P（A）通过全概率公式可得：
​				P(A) = 全加（i:1->n）P（B~i~）*P（A|B~i~）
​	3.特征条件独立假设：
​		给定训练集，其中每个样本都包括n维特征，类标记集合含有K种类别。如果现在来一个新样本这么判断类别？
​		这个问题就是给定x它属于那个类别的概率更大。那么问题就转化为求解P(y1|x),p(y2|x)...p(yl|x)那个最大。
​			输出最大概率即可。arg max P(y~k~|x)
​			P(y~k~|x)采用贝叶斯定理求得：
​				P(y~k~|x)=P(x|y~k~)*p(y~k~) / P(x)
​			代入全概率公式：
​				P(y~k~|x)=P(x|y~k~)*p(y~k~) / 全加(i:1->n)P(x|y~k~)*P(y~k~)
​			P(y~k~)根据训练集求出
​			P(x|y~k~)：假设每个特征都相互独立即可化为：P(x|y~k~)=连乘(i:1->n)P(x~i~|y~i~)

P(y~k~|x)=P(x|y~k~)*p(y~k~) / 全加(i:1->n)(P(y~k~)*连乘(i:1->n)P(x~i~|y~i~)）
	朴素贝叶斯最终表示为：
		f(x)=arg max（P(y~k~)*连乘(i:1->n)P(xi|y~i~)）			

参数估计：1.极大似然估计。2.贝叶斯估计