---------------------------------------------------------------
## 随机森林算法:（分类算法）

​	用随机的方式建立一个森林，森林里面有很多的决策树。每一个决策树是没有关联的。得到决策树后每当有一个新的样本进入的时候就让森林中的每一颗决策树分别进行一下判断，然后看看哪一类被选择最多就预测为哪一类。

### 	优点：

​		  1.能处理高纬度的数据，并且可以不用做特征选择
​		  2.训练完成后能给出那些feature比较重要（越靠近数根越重要）
​		  3.易做并行运算，运行速度快
​		  4.可以做可视化展示

### 随机森林的生成：

​	1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中抽取N个训练样本
（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本
​	2）如果每个样本的特征维度为M，指定一个常数m，且 m<M ，随机地从M个特征中选取m个特征子集每次树进行分裂时，从这m个特征中选择最优的。
​	3）每棵树都尽可能最大程度地生长，切没有剪枝过程。

### 随机森林的分类效果相关因素：

​	1）任意两棵树的相关性：相关性越大，错误率越大。
​	2）每棵树的分类能力：分类能力越强，错误率越低。
​	特征个数m减小时树的相关性和分类能力也相应减小。m增大时两个因素也会增大。m应该选择最优或者比较合适的范围

### 特征个数m的选择:

​	计算袋外误差率oob erroe。根据袋外误差率确定m。
​	oob erroe的计算：
​		1）对每个样本计算它作为oob样本的树对它的分类情况
​		2）以简单多数投票作为该样本的分类结果
​		3）最后用误分个数占样本总数的比率作为随机森林的oob误分率
​	当oob误分率最低时就是对应特征个数m的最优值