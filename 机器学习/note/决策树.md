## 决策树算法：

​	分类方法。<u>从根节点开始一步步走到叶子节点。</u>
​	决策树模型具有可读性、分类速度快。
​	在决策树中上层节点应比下次节点的区分好。
​	对于区分的度量：熵（随机变量的不确定性。即混乱程度）对于决策树希望熵值小
​	不同过程的熵的降低值为增益。区分那种选择的区分度大一般使用增益。

### 	ID~3~算法：

​		使用信息增益率（增益比上自身熵值）作为衡量标准的算法；
​		从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特
​		征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决
​		策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相
​		当于用极大似然法进行概率模型的选择。但是ID3算法只有树的生成，所以该算法生成的树容易
​		产生过拟合。

### 	C4.5算法：

​		使用信息增益比（增益比上自身熵值）作为衡量标准与ID3基本类似

### 	CART算法（分类树与回归树）：

​				使用GINI系数作为衡量标准。步骤：1.决策树生成。2.决策树剪枝

### 决策树的剪枝：

​	决策树算法容易造成过拟合。决策树在学习是过多的考虑对训练数据的正确分类而构建出过于复杂的决策树。对已经生成的决策树进行简化，即剪枝。